{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting and EDA for Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "from dotenv import dotenv_values\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from typing import Tuple\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting env var\n",
    "secrets = dotenv_values()\n",
    "\n",
    "host =secrets['DB_HOST']\n",
    "name = secrets['DB_NAME']\n",
    "user = secrets['DB_USER']\n",
    "pwd = secrets['DB_PWD']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7043\n"
     ]
    }
   ],
   "source": [
    "# Connecting to DB\n",
    "\n",
    "connection = pymysql.connect(\n",
    "    host = host,\n",
    "    user = user,\n",
    "    password = pwd,\n",
    "    database = name\n",
    "    )\n",
    "\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"SELECT * FROM churn_status\")\n",
    "\n",
    "results = cursor.fetchall()\n",
    "\n",
    "print(len(results))\n",
    "\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "Done from local data files\n",
    "\n",
    "---\n",
    "\n",
    "To Do List\n",
    "- Join unscaled and unprocessed datasets. \n",
    "- Remove rows without churn label (if any)\n",
    "- Check for class imbalance -> if have, perform stratified split \n",
    "- Split into train-validation (70-30)\n",
    "- Perform outlier removal only on train set\n",
    "- After outlier removal, preprocess (categorical encode and scale) columns in both datasets\n",
    "- Perform PCA to determine which features to use\n",
    "- Train machine learning model with kfold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "ACC_PATH = \"data_given/1_account.csv\"\n",
    "ACC_USAGE_PATH = \"data_given/2_account_usage.csv\"\n",
    "CHURN_STATUS_PATH = \"data_given/3_churn_status.csv\"\n",
    "CITY_PATH = \"data_given/4_city.csv\"\n",
    "CUSTOMER_PATH = \"data_given/5_customer.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining all unprocessed tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading DFs\n",
    "CUSTOMER_DF = pd.read_csv(CUSTOMER_PATH)\n",
    "ACCOUNT_DF = pd.read_csv(ACC_PATH)\n",
    "ACC_USAGE_DF = pd.read_csv(ACC_USAGE_PATH)\n",
    "CHURN_STATUS_DF = pd.read_csv(CHURN_STATUS_PATH)\n",
    "CITY_DF = pd.read_csv(CITY_PATH)\n",
    "\n",
    "# Joining customer and account tables on customer_id\n",
    "DF = pd.merge(CUSTOMER_DF, ACCOUNT_DF, on='customer_id', how='inner')\n",
    "# Joining df with account_usage_df on account_id\n",
    "DF = pd.merge(DF, ACC_USAGE_DF, on='account_id', how='inner')\n",
    "# Joining df with churn_status_df on customer_id\n",
    "DF = pd.merge(DF, CHURN_STATUS_DF, on='customer_id', how='inner')\n",
    "# Joining df with city_df on area_id\n",
    "DF = pd.merge(DF, CITY_DF, on='zip_code', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['customer_id', 'gender', 'age', 'senior_citizen', 'married',\n",
       "       'num_dependents', 'zip_code', 'account_id', 'tenure_months',\n",
       "       'num_referrals', 'has_internet_service', 'internet_type',\n",
       "       'has_unlimited_data', 'has_phone_service', 'has_multiple_lines',\n",
       "       'has_premium_tech_support', 'has_online_security', 'has_online_backup',\n",
       "       'has_device_protection', 'contract_type', 'paperless_billing',\n",
       "       'payment_method', 'avg_long_distance_fee_monthly',\n",
       "       'total_long_distance_fee', 'avg_gb_download_monthly', 'stream_tv',\n",
       "       'stream_movie', 'stream_music', 'total_monthly_fee',\n",
       "       'total_charges_quarter', 'total_refunds', 'status', 'churn_label',\n",
       "       'churn_category', 'churn_reason', 'area_id', 'city', 'latitutde',\n",
       "       'longitude', 'population'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.head()\n",
    "DF.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"customer_id\":\"0002-ORFBO\",\"gender\":\"Female\",\"age\":37,\"senior_citizen\":\"No\",\"married\":\"Yes\",\"num_dependents\":0,\"zip_code\":93225,\"account_id\":\"XSWV-PAYXZ\",\"tenure_months\":9,\"num_referrals\":2,\"has_internet_service\":\"Yes\",\"internet_type\":\"Cable\",\"has_unlimited_data\":\"Yes\",\"has_phone_service\":\"Yes\",\"has_multiple_lines\":\"No\",\"has_premium_tech_support\":\"Yes\",\"has_online_security\":\"No\",\"has_online_backup\":\"Yes\",\"has_device_protection\":\"No\",\"contract_type\":\"One Year\",\"paperless_billing\":\"Yes\",\"payment_method\":\"Credit Card\",\"avg_long_distance_fee_monthly\":42.39,\"total_long_distance_fee\":381.51,\"avg_gb_download_monthly\":16,\"stream_tv\":\"Yes\",\"stream_movie\":\"No\",\"stream_music\":\"No\",\"total_monthly_fee\":65.6,\"total_charges_quarter\":593.3,\"total_refunds\":0.0,\"status\":\"Stayed\",\"churn_label\":\"No\",\"churn_category\":null,\"churn_reason\":null,\"area_id\":649,\"city\":\"Frazier Park\",\"latitutde\":34.827662,\"longitude\":-118.999073,\"population\":4498}]\n"
     ]
    }
   ],
   "source": [
    "DF.head(1)\n",
    "# Assuming you have a DataFrame called df\n",
    "first_row_json = DF.head(1).to_json(orient='records')\n",
    "\n",
    "print(first_row_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import BinaryEncoder\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\n",
    "def convert_to_categorical(df:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts the following columns to categorical data\n",
    "    \n",
    "    Yes No to 1/0\n",
    "    - has_internet_service\n",
    "    - has_phone_service\n",
    "    - has_unlimited_data\n",
    "    - has_multiple_lines\n",
    "    - has_premium_tech_support\n",
    "    - has_online_security\n",
    "    - has_online_backup\n",
    "    - has_device_protection\n",
    "    - paperless_billing\n",
    "    - stream_movie\n",
    "    - stream_music\n",
    "    - stream_tv\n",
    "    - churn_label\n",
    "    - senior_citizen\n",
    "    - married\n",
    "    - gender (1 for male, 0 for female)\n",
    "\n",
    "    One-hot encodes the following columns\n",
    "    - payment_method\n",
    "    - internet_type\n",
    "\n",
    "    Ordinal encodes the following columns\n",
    "    - contract_type (one-year, month-to-month, two-year) => (1,0,2) \n",
    "\n",
    "    Label Encodes the following columns\n",
    "    - status\n",
    "    \n",
    "    Binary encodes\n",
    "    - churn_category\n",
    "\n",
    "    \"\"\"\n",
    "    # Creating output_df\n",
    "    output_df = df.copy()\n",
    "\n",
    "    # Copying over account_id, customer_id\n",
    "    output_df['account_id'] = df['account_id']\n",
    "    output_df['customer_id'] = df['customer_id']\n",
    "    output_df['zip_code'] = df['zip_code']\n",
    "\n",
    "     # Converting yes/no to 1/0\n",
    "    mapping = {'Yes':1, 'No':0, \"Male\":1, \"Female\":0}\n",
    "    output_df['has_internet_service'] = df['has_internet_service'].map(mapping)\n",
    "    output_df['has_phone_service'] = df['has_phone_service'].map(mapping)\n",
    "    output_df['has_unlimited_data'] = df['has_unlimited_data'].map(mapping)\n",
    "    output_df['has_multiple_lines'] = df['has_multiple_lines'].map(mapping)\n",
    "    output_df['has_premium_tech_support'] = df['has_premium_tech_support'].map(mapping)\n",
    "    output_df['has_online_security'] = df['has_online_security'].map(mapping)\n",
    "    output_df['has_online_backup'] = df['has_online_backup'].map(mapping)\n",
    "    output_df['has_device_protection'] = df['has_device_protection'].map(mapping)\n",
    "    output_df['paperless_billing'] = df['paperless_billing'].map(mapping)\n",
    "    output_df['stream_movie'] = df['stream_movie'].map(mapping)\n",
    "    output_df['stream_music'] = df['stream_music'].map(mapping)\n",
    "    output_df['stream_tv'] = df['stream_tv'].map(mapping)\n",
    "    output_df['senior_citizen'] = df['senior_citizen'].map(mapping)\n",
    "    output_df['married'] = df['married'].map(mapping)\n",
    "    output_df['gender'] = df['gender'].map(mapping)\n",
    "\n",
    "    # Some are missing churn_labels, but have status as \"churned\"\n",
    "    output_df['churn_label'] = df['churn_label']\n",
    "    output_df.loc[df['status'] == 'Churned', 'churn_label'] = \"Yes\"\n",
    "    output_df['churn_label'] = output_df['churn_label'].map(mapping)\n",
    "    # output_df['churn_label'] = output_df['churn_label'].fillna(0)\n",
    "\n",
    "    # Ordinal Encoding\n",
    "    mapping = {'Month-to-Month':0, 'One Year':1, 'Two Year':2}\n",
    "    output_df['contract_type'] = df['contract_type'].map(mapping)\n",
    "\n",
    "    # One-hot encoding\n",
    "    one_hot_encoder = OneHotEncoder()\n",
    "    one_hot_encoded_cols = [\"payment_method\", \"internet_type\"]\n",
    "    encoded_features = one_hot_encoder.fit_transform(df[one_hot_encoded_cols])\n",
    "    encoded_df = pd.DataFrame(encoded_features.toarray(), columns=one_hot_encoder.get_feature_names_out(one_hot_encoded_cols))\n",
    "    output_df = pd.concat([output_df, encoded_df], axis=1)\n",
    "\n",
    "    # Label Encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    output_df['status'] = label_encoder.fit_transform(df['status'])\n",
    "\n",
    "    # Binary Encoding\n",
    "    binary_encoder = BinaryEncoder(cols=['churn_category'])\n",
    "    binary_encoder.fit_transform(df['churn_category'])\n",
    "    churn_cat = binary_encoder.transform(df['churn_category'])\n",
    "    output_df = pd.concat([output_df, churn_cat], axis=1)\n",
    "\n",
    "    # Copying over the numerical values\n",
    "    output_df['num_referrals'] = df['num_referrals']\n",
    "    output_df['tenure_months'] = df['tenure_months']\n",
    "    output_df['avg_long_distance_fee_monthly'] = df[['avg_long_distance_fee_monthly']]\n",
    "    output_df['total_long_distance_fee'] = df[['total_long_distance_fee']]\n",
    "    output_df['avg_gb_download_monthly'] = df[['avg_gb_download_monthly']]\n",
    "    output_df['total_monthly_fee'] = df[['total_monthly_fee']]\n",
    "    output_df['total_charges_quarter'] = df[['total_charges_quarter']]\n",
    "    output_df['total_refunds'] = df[['total_refunds']]\n",
    "    output_df['population'] = df[['population']]\n",
    "\n",
    "    # Dropping tables\n",
    "    output_df = output_df.drop(columns=['internet_type', 'payment_method', 'churn_category', 'churn_reason'])\n",
    "\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_numerical_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, MinMaxScaler]:\n",
    "    \"\"\"\n",
    "    Scales all numerical data\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    numerical_cols = ['num_referrals', 'age', 'tenure_months', 'avg_long_distance_fee_monthly', 'total_long_distance_fee', 'avg_gb_download_monthly', 'total_monthly_fee', 'total_charges_quarter', 'total_refunds', 'population']\n",
    "    df.loc[:, numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "    return df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def perform_pca(df: pd.DataFrame, n_components: int):\n",
    "    # Separate the features from the target variable (if applicable)\n",
    "    features = df.drop(columns=['churn_label', 'status', 'customer_id', 'account_id', 'zip_code', 'city'])  # Replace 'target' with your churn_label variable column name\n",
    "\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    transformed_features = pca.fit_transform(features)\n",
    "\n",
    "    # Create a new DataFrame with the transformed features\n",
    "    transformed_df = pd.DataFrame(data=transformed_features, columns=[f\"PC{i+1}\" for i in range(n_components)])\n",
    "\n",
    "    # Concatenate the transformed features with the churn_label variable (if applicable)\n",
    "    if 'churn_label' in df.columns:\n",
    "        transformed_df['churn_label'] = df['churn_label']\n",
    "\n",
    "    return transformed_df, pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming DF\n",
    "parsed_df = convert_to_categorical(DF)\n",
    "parsed_df, scaler = scale_numerical_data(parsed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>churn_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-118.052726</td>\n",
       "      <td>0.691531</td>\n",
       "      <td>0.090287</td>\n",
       "      <td>0.159955</td>\n",
       "      <td>-0.551817</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-118.052763</td>\n",
       "      <td>0.766318</td>\n",
       "      <td>-0.866742</td>\n",
       "      <td>-0.265651</td>\n",
       "      <td>0.058195</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-118.052813</td>\n",
       "      <td>0.820702</td>\n",
       "      <td>-1.377473</td>\n",
       "      <td>0.542717</td>\n",
       "      <td>0.278057</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-118.052726</td>\n",
       "      <td>0.737594</td>\n",
       "      <td>-0.494861</td>\n",
       "      <td>0.214424</td>\n",
       "      <td>-0.427950</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-583.050469</td>\n",
       "      <td>-0.876171</td>\n",
       "      <td>-0.075605</td>\n",
       "      <td>-0.776590</td>\n",
       "      <td>-0.251477</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7038</th>\n",
       "      <td>-158.052109</td>\n",
       "      <td>0.217740</td>\n",
       "      <td>0.109590</td>\n",
       "      <td>-1.088779</td>\n",
       "      <td>-0.241642</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7039</th>\n",
       "      <td>723.964045</td>\n",
       "      <td>1.023746</td>\n",
       "      <td>-0.544855</td>\n",
       "      <td>0.821999</td>\n",
       "      <td>-0.841849</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7040</th>\n",
       "      <td>723.963982</td>\n",
       "      <td>1.034464</td>\n",
       "      <td>-0.454879</td>\n",
       "      <td>-1.056443</td>\n",
       "      <td>0.211924</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7041</th>\n",
       "      <td>723.964015</td>\n",
       "      <td>1.047806</td>\n",
       "      <td>-0.501376</td>\n",
       "      <td>-0.728685</td>\n",
       "      <td>-0.092571</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7042</th>\n",
       "      <td>723.964160</td>\n",
       "      <td>0.825187</td>\n",
       "      <td>1.804288</td>\n",
       "      <td>0.205775</td>\n",
       "      <td>-0.525370</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7043 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             PC1       PC2       PC3       PC4       PC5  churn_label\n",
       "0    -118.052726  0.691531  0.090287  0.159955 -0.551817            0\n",
       "1    -118.052763  0.766318 -0.866742 -0.265651  0.058195            0\n",
       "2    -118.052813  0.820702 -1.377473  0.542717  0.278057            0\n",
       "3    -118.052726  0.737594 -0.494861  0.214424 -0.427950            0\n",
       "4    -583.050469 -0.876171 -0.075605 -0.776590 -0.251477            0\n",
       "...          ...       ...       ...       ...       ...          ...\n",
       "7038 -158.052109  0.217740  0.109590 -1.088779 -0.241642            0\n",
       "7039  723.964045  1.023746 -0.544855  0.821999 -0.841849            0\n",
       "7040  723.963982  1.034464 -0.454879 -1.056443  0.211924            0\n",
       "7041  723.964015  1.047806 -0.501376 -0.728685 -0.092571            1\n",
       "7042  723.964160  0.825187  1.804288  0.205775 -0.525370            0\n",
       "\n",
       "[7043 rows x 6 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_df, pca = perform_pca(parsed_df, 5)\n",
    "transformed_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model\n",
    "* Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Imbalance Ratio in Training Set: 0.3611983570910848\n",
      "Class Imbalance Ratio after Undersampling: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Split the transformed_df into features (X) and target variable (y)\n",
    "X = transformed_df.drop('churn_label', axis=1)\n",
    "y = transformed_df['churn_label']\n",
    "\n",
    "# Perform train-test split while maintaining the class imbalance\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Check the class distribution in the training set\n",
    "class_counts = y_train.value_counts()\n",
    "class_imbalance_ratio = class_counts[1] / class_counts[0]\n",
    "print(\"Class Imbalance Ratio in Training Set:\", class_imbalance_ratio)\n",
    "\n",
    "\n",
    "# Create an instance of RandomUnderSampler\n",
    "undersampler = RandomUnderSampler(sampling_strategy=0.5, random_state=42)\n",
    "\n",
    "# Resample the training data\n",
    "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the class distribution after undersampling\n",
    "class_counts_resampled = pd.Series(y_train_resampled).value_counts()\n",
    "class_imbalance_ratio_resampled = class_counts_resampled[1] / class_counts_resampled[0]\n",
    "print(\"Class Imbalance Ratio after Undersampling:\", class_imbalance_ratio_resampled)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Train Accuracy = 0.9986064659977704, Validation Accuracy = 0.8929765886287625\n",
      "Fold 2: Train Accuracy = 0.9969342251950948, Validation Accuracy = 0.89520624303233\n",
      "Fold 3: Train Accuracy = 0.9983277591973244, Validation Accuracy = 0.9141583054626533\n",
      "Fold 4: Train Accuracy = 0.9980490523968785, Validation Accuracy = 0.8974358974358975\n",
      "Fold 5: Train Accuracy = 0.9980490523968785, Validation Accuracy = 0.9018952062430323\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Set up XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "# Set up StratifiedKFold with class imbalance\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store the evaluation results for each fold\n",
    "fold_train_scores = []\n",
    "fold_val_scores = []\n",
    "\n",
    "# Iterate over the folds\n",
    "# for train_index, val_index in kfold.split(X_train, y_train):\n",
    "for train_index, val_index in kfold.split(X_train_resampled, y_train_resampled):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Train the model\n",
    "    xgb_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Predict on the training and validation data\n",
    "    y_train_pred = xgb_model.predict(X_train_fold)\n",
    "    y_val_pred = xgb_model.predict(X_val_fold)\n",
    "\n",
    "    # Calculate accuracy scores\n",
    "    train_accuracy = accuracy_score(y_train_fold, y_train_pred)\n",
    "    val_accuracy = accuracy_score(y_val_fold, y_val_pred)\n",
    "\n",
    "    # Append scores to the lists\n",
    "    fold_train_scores.append(train_accuracy)\n",
    "    fold_val_scores.append(val_accuracy)\n",
    "\n",
    "# Print the evaluation results for each fold\n",
    "for i in range(len(fold_train_scores)):\n",
    "    print(f\"Fold {i+1}: Train Accuracy = {fold_train_scores[i]}, Validation Accuracy = {fold_val_scores[i]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8800567778566359\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "y_test_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the test accuracy\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance\n",
    "Not churned = 5174 ~ 75%\n",
    "\n",
    "Churned = 1817 ~25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "churn_label\n",
      "No    5174\n",
      "1     1869\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking for class imbalance\n",
    "print(parsed_df['churn_label'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_zscore(df:pd.DataFrame, threshold:int=3)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Detect outliers using the z-score method.\n",
    "    Returns a boolean mask indicating the outliers.\n",
    "    \"\"\"\n",
    "    z_scores = (df - np.mean(df)) / np.std(df)\n",
    "    outliers = np.abs(z_scores) > threshold\n",
    "    return outliers\n",
    "\n",
    "def plot_boxplot(df_column:pd.Series)->None:\n",
    "    \"\"\"\n",
    "    Plots a box plot based on the given DataFrame column using Plotly.\n",
    "    \"\"\"\n",
    "    # Plotting boxplot using Plotly\n",
    "    fig = px.box(df_column, title='Box Plot')\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "\n",
    "def get_value_at_percentile(df:pd.DataFrame, column:str, percentile:int)->float:\n",
    "    \"\"\"\n",
    "    Returns the value at a specific percentile in a DataFrame column.\n",
    "    \"\"\"\n",
    "    # Get the values from the specified column\n",
    "    column_values = df[column].values\n",
    "\n",
    "    # Calculate the percentile value\n",
    "    value_at_percentile = np.percentile(column_values, percentile)\n",
    "\n",
    "    return value_at_percentile\n",
    "\n",
    "def remove_rows_by_values(df:pd.DataFrame, column:str, lower_bound:float, upper_bound:float):\n",
    "    \"\"\"\n",
    "    Removes rows from a DataFrame based on specific column values.\n",
    "    \"\"\"\n",
    "    # Create a boolean mask to filter rows with the specified column values\n",
    "    mask = (df[column] >= lower_bound) & (df[column] <= upper_bound)\n",
    "\n",
    "    # Filter the DataFrame using the boolean mask\n",
    "    updated_df = df[mask]\n",
    "\n",
    "    return updated_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Analysis Summary\n",
    "### ACC_DF\n",
    "\n",
    "1. tenure_months\n",
    "\n",
    "In the dataset, 362 rows have tenure_months at 72 while 0 rows have tenure_months at 71. As 72 months is 3 years, this suggests that the maximum contract period is 3 years. 362 rows consists of approximately 5% of the entire data set we have currently, hence I will not be removing any outliers based on tenure_months.\n",
    "\n",
    "2. num_referrals\n",
    "\n",
    "I decide to keep the 0th to 95th percentile of the rows based on the number of referrals. This meant the cutoff value would be 9, and 225 rows will be removed as a results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACC_DF = pd.read_csv(ACC_PATH)\n",
    "# ACC_DF.head()\n",
    "# # Keeping from 80th percentile to 0th percentile for tenure_months\n",
    "# # plot_boxplot(ACC_DF['num_referrals'])\n",
    "# get_value_at_percentile(ACC_DF, 'num_referrals', 80)\n",
    "\n",
    "# def acc_df_remove_outliers(df:pd.DataFrame):\n",
    "#     # Tenure_months: Keep from 0th to 80th percentile\n",
    "#     # Num_referrals: Keep from 0th to 80th percentile\n",
    "#     tenure_months_upper_bound = get_value_at_percentile(df, 'tenure_months', 90)\n",
    "#     num_referarals_upper_bound = get_value_at_percentile(df, 'num_referrals', 80)\n",
    "\n",
    "#     # Removing outliers\n",
    "#     output_df = remove_rows_by_values(df, 'tenure_months', 0, tenure_months_upper_bound)\n",
    "#     print(f\"Removed {len(df) - len(output_df)} rows from the DataFrame based on the 'tenure_months' column values.\")\n",
    "\n",
    "#     output_df = remove_rows_by_values(df, 'num_referrals', 0, num_referarals_upper_bound)\n",
    "#     print(f\"Removed {len(df) - len(output_df)} rows from the DataFrame based on the 'num_referrals' column values.\")\n",
    "\n",
    "#     return output_df\n",
    "\n",
    "# plot_boxplot(ACC_DF['tenure_months'])\n",
    "# cleaned_acc_df = acc_df_remove_outliers(ACC_DF)\n",
    "# plot_boxplot(cleaned_acc_df['tenure_months'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_values(df:pd.DataFrame)->dict:\n",
    "    \"\"\"\n",
    "    Runs through the columns of a dataframe and prints the unique values of each column. \n",
    "    \"\"\"\n",
    "    dict_unique_values = {}\n",
    "    for cols in df.columns:\n",
    "        dict_unique_values[cols] = df[cols].unique()\n",
    "    return dict_unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NaN_count(df:pd.DataFrame)->dict:\n",
    "    \"\"\"\n",
    "    Returns the number of NaN values for each column in a dictionary.\n",
    "    \"\"\"\n",
    "    nan_count = df.isna().sum().to_dict()\n",
    "    return nan_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_acc_df(df:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocesses the account dataframe as below:\n",
    "\n",
    "    Converts follow columns (yes/no) to 1/0\n",
    "    - has_internet_service\n",
    "    - has_phone_service\n",
    "    - has_unlimited_data\n",
    "    - has_multiple_lines\n",
    "    - has_premium_tech_support\n",
    "    - has_online_security\n",
    "    - has_online_backup\n",
    "    - has_device_protection\n",
    "    - paperless_billing\n",
    "\n",
    "    Converts follow columns to ordinal values:\n",
    "    - contract_type (one-year, month-to-month, two-year) => (1,0,2) \n",
    "\n",
    "    One-hot encodes following columns:\n",
    "    - payment_method\n",
    "    - internet_type\n",
    "\n",
    "    Scales following columns:\n",
    "    - tenure_months\n",
    "    \"\"\"\n",
    "    # Creating a new df\n",
    "    output_df = pd.DataFrame()\n",
    "\n",
    "    # Copying over account_id\tcustomer_id\ttenure_months\n",
    "    output_df['account_id'] = df['account_id']\n",
    "    output_df['customer_id'] = df['customer_id']\n",
    "\n",
    "    # Converting yes/no to 1/0\n",
    "    mapping = {'Yes':1, 'No':0}\n",
    "\n",
    "    output_df['has_internet_service'] = df['has_internet_service'].map(mapping)\n",
    "    output_df['has_phone_service'] = df['has_phone_service'].map(mapping)\n",
    "    output_df['has_unlimited_data'] = df['has_unlimited_data'].map(mapping)\n",
    "    output_df['has_multiple_lines'] = df['has_multiple_lines'].map(mapping)\n",
    "    output_df['has_premium_tech_support'] = df['has_premium_tech_support'].map(mapping)\n",
    "    output_df['has_online_security'] = df['has_online_security'].map(mapping)\n",
    "    output_df['has_online_backup'] = df['has_online_backup'].map(mapping)\n",
    "    output_df['has_device_protection'] = df['has_device_protection'].map(mapping)\n",
    "    output_df['paperless_billing'] = df['paperless_billing'].map(mapping)\n",
    "\n",
    "    mapping = {'Month-to-Month':0, 'One Year':1, 'Two Year':2}\n",
    "    output_df['contract_type'] = df['contract_type'].map(mapping)\n",
    "\n",
    "    # One-hot encoding\n",
    "    # one_hot_encoder = OneHotEncoder()\n",
    "    # one_hot_encoded_cols = [\"payment_method\", \"internet_type\"]\n",
    "    # encoded_df = pd.DataFrame(one_hot_encoder.fit_transform(df[one_hot_encoded_cols]))\n",
    "    # encoded_df.columns = one_hot_encoder.get_feature_names_out(one_hot_encoded_cols)\n",
    "    # print(encoded_df.head())\n",
    "\n",
    "    # One-hot encoding\n",
    "    one_hot_encoder = OneHotEncoder()\n",
    "    one_hot_encoded_cols = [\"payment_method\", \"internet_type\"]\n",
    "    encoded_features = one_hot_encoder.fit_transform(df[one_hot_encoded_cols])\n",
    "    encoded_df = pd.DataFrame(encoded_features.toarray(), columns=one_hot_encoder.get_feature_names_out(one_hot_encoded_cols))\n",
    "    output_df = pd.concat([output_df, encoded_df], axis=1)\n",
    "\n",
    "    # Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    output_df['tenure_months'] = scaler.fit_transform(df[['tenure_months']])\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Account Usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_acc_usage_df(df:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the account usage dataframe as below:\n",
    "    Scale the following columns:\n",
    "    - avg_long_distance_fee_monthly\n",
    "    - total_long_distance_fee\n",
    "    - avg_gb_download_monthly\n",
    "    - total_monthly_fee\n",
    "    - total_chargers_quarter\n",
    "    - total_refunds\n",
    "    Converts following col to 1/0:\n",
    "    - stream_move\n",
    "    - stream_music\n",
    "    - stream_tv\n",
    "    \"\"\"\n",
    "    # Create new df\n",
    "    output_df = pd.DataFrame()\n",
    "\n",
    "    # Copying over acc_id\n",
    "    output_df['account_id'] = df['account_id']\n",
    "\n",
    "    # Converting yes/no to 1/0\n",
    "    mapping = {'Yes':1, 'No':0}\n",
    "\n",
    "    output_df['stream_movie'] = df['stream_movie'].map(mapping)\n",
    "    output_df['stream_music'] = df['stream_music'].map(mapping)\n",
    "    output_df['stream_tv'] = df['stream_tv'].map(mapping)\n",
    "\n",
    "    # Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    output_df['avg_long_distance_fee_monthly'] = scaler.fit_transform(df[['avg_long_distance_fee_monthly']])\n",
    "    output_df['total_long_distance_fee'] = scaler.fit_transform(df[['total_long_distance_fee']])\n",
    "    output_df['avg_gb_download_monthly'] = scaler.fit_transform(df[['avg_gb_download_monthly']])\n",
    "    output_df['total_monthly_fee'] = scaler.fit_transform(df[['total_monthly_fee']])\n",
    "    output_df['total_charges_quarter'] = scaler.fit_transform(df[['total_charges_quarter']])\n",
    "    output_df['total_refunds'] = scaler.fit_transform(df[['total_refunds']])\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Churn Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocess_churn_df(df:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the churn status dataframe as below:\n",
    "    Convert following col to 1/0:\n",
    "    - churn_label\n",
    "\n",
    "    Binary encodes:\n",
    "    - churn_category\n",
    "    \n",
    "    Label-encodes:\n",
    "    - status\n",
    "\n",
    "    Drops the following:\n",
    "    - churn_reason -> Not planning to do NLP\n",
    "    \"\"\"\n",
    "    # Create new df\n",
    "    output_df = pd.DataFrame()\n",
    "\n",
    "    # Copying over acc_id\n",
    "    output_df['customer_id'] = df['customer_id']\n",
    "\n",
    "    # Converting yes/no to 1/0\n",
    "    # Additional step to check status \n",
    "    output_df['churn_label'] = df['churn_label'].fillna(0)\n",
    "    output_df.loc[df['status'] == 'Churned', 'churn_label'] = 1\n",
    "    mapping = {'Yes':1, 'No':0, }\n",
    "    output_df['churn_label'] = df['churn_label'].map(mapping)\n",
    "\n",
    "    # Binary Encoding\n",
    "    binary_encoder = BinaryEncoder(cols=['churn_category'])\n",
    "    binary_encoder.fit_transform(df['churn_category'])\n",
    "    churn_cat = binary_encoder.transform(df['churn_category'])\n",
    "    output_df = pd.concat([output_df, churn_cat], axis=1)\n",
    "\n",
    "    # Label Encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    output_df['status'] = label_encoder.fit_transform(df['status'])\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_city_df(df:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the city dataframe as below:\n",
    "    Min-max scales population column\n",
    "    Keep area_id and population only\n",
    "    \"\"\"\n",
    "    # Creating new df\n",
    "    output_df = pd.DataFrame()\n",
    "\n",
    "    # Copying over area_id\n",
    "    output_df['zip_code'] = df['zip_code']\n",
    "\n",
    "    # Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    output_df['population'] = scaler.fit_transform(df[['population']])\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_customer_df(df:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the customer dataframe as below:\n",
    "    Convert following col to 1/0:\n",
    "    - senior_citizen\n",
    "    - married\n",
    "    - gender (1 for male, 0 for female)\n",
    "    \"\"\"\n",
    "    # Create new df\n",
    "    output_df = pd.DataFrame()\n",
    "\n",
    "    # Copying over customer_id, zip_code\n",
    "    output_df['customer_id'] = df['customer_id']\n",
    "    output_df['zip_code'] = df['zip_code']\n",
    "\n",
    "    # Converting columns to 1/0\n",
    "    mapping = {'Yes':1, 'No':0, \"Male\":1, \"Female\":0}\n",
    "    output_df['senior_citizen'] = df['senior_citizen'].map(mapping)\n",
    "    output_df['married'] = df['married'].map(mapping)\n",
    "    output_df['gender'] = df['gender'].map(mapping)\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing all CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dfs from CSV\n",
    "ACCOUNT_DF = pd.read_csv(ACC_PATH)\n",
    "ACCOUNT_USAGE_DF = pd.read_csv(ACC_USAGE_PATH)\n",
    "CHURN_STATUS_DF = pd.read_csv(CHURN_STATUS_PATH)\n",
    "CITY_DF = pd.read_csv(CITY_PATH)\n",
    "CUSTOMER_DF = pd.read_csv(CUSTOMER_PATH)\n",
    "\n",
    "# Preprocessing dfs\n",
    "account_df = preprocess_acc_df(ACCOUNT_DF)\n",
    "account_usage_df = preprocess_acc_usage_df(ACCOUNT_USAGE_DF)\n",
    "churn_status_df = preprocess_churn_df(CHURN_STATUS_DF)\n",
    "city_df = preprocess_city_df(CITY_DF)\n",
    "customer_df = preprocess_customer_df(CUSTOMER_DF)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining customer and account tables on customer_id\n",
    "df = pd.merge(customer_df, account_df, on='customer_id', how='inner')\n",
    "# Joining df with account_usage_df on account_id\n",
    "df = pd.merge(df, account_usage_df, on='account_id', how='inner')\n",
    "# Joining df with churn_status_df on customer_id\n",
    "df = pd.merge(df, churn_status_df, on='customer_id', how='inner')\n",
    "# Joining df with city_df on area_id\n",
    "df = pd.merge(df, city_df, on='zip_code', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai300",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
