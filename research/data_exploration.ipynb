{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting and EDA for Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "from dotenv import dotenv_values\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from typing import Tuple\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting env var\n",
    "secrets = dotenv_values()\n",
    "\n",
    "host =secrets['DB_HOST']\n",
    "name = secrets['DB_NAME']\n",
    "user = secrets['DB_USER']\n",
    "pwd = secrets['DB_PWD']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7043\n"
     ]
    }
   ],
   "source": [
    "# Connecting to DB\n",
    "\n",
    "connection = pymysql.connect(\n",
    "    host = host,\n",
    "    user = user,\n",
    "    password = pwd,\n",
    "    database = name\n",
    "    )\n",
    "\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"SELECT * FROM churn_status\")\n",
    "\n",
    "results = cursor.fetchall()\n",
    "\n",
    "print(len(results))\n",
    "\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "Done from local data files\n",
    "\n",
    "---\n",
    "\n",
    "To Do List\n",
    "- Join unscaled and unprocessed datasets. \n",
    "- Remove rows without churn label (if any)\n",
    "- Check for class imbalance -> if have, perform stratified split \n",
    "- Split into train-validation (70-30)\n",
    "- Perform outlier removal only on train set\n",
    "- After outlier removal, preprocess (categorical encode and scale) columns in both datasets\n",
    "- Perform PCA to determine which features to use\n",
    "- Train machine learning model with kfold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "ACC_PATH = \"data_given/1_account.csv\"\n",
    "ACC_USAGE_PATH = \"data_given/2_account_usage.csv\"\n",
    "CHURN_STATUS_PATH = \"data_given/3_churn_status.csv\"\n",
    "CITY_PATH = \"data_given/4_city.csv\"\n",
    "CUSTOMER_PATH = \"data_given/5_customer.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining all unprocessed tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading DFs\n",
    "CUSTOMER_DF = pd.read_csv(CUSTOMER_PATH)\n",
    "ACCOUNT_DF = pd.read_csv(ACC_PATH)\n",
    "ACC_USAGE_DF = pd.read_csv(ACC_USAGE_PATH)\n",
    "CHURN_STATUS_DF = pd.read_csv(CHURN_STATUS_PATH)\n",
    "CITY_DF = pd.read_csv(CITY_PATH)\n",
    "\n",
    "# Joining customer and account tables on customer_id\n",
    "DF = pd.merge(CUSTOMER_DF, ACCOUNT_DF, on='customer_id', how='inner')\n",
    "# Joining df with account_usage_df on account_id\n",
    "DF = pd.merge(DF, ACC_USAGE_DF, on='account_id', how='inner')\n",
    "# Joining df with churn_status_df on customer_id\n",
    "DF = pd.merge(DF, CHURN_STATUS_DF, on='customer_id', how='inner')\n",
    "# Joining df with city_df on area_id\n",
    "DF = pd.merge(DF, CITY_DF, on='zip_code', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>senior_citizen</th>\n",
       "      <th>married</th>\n",
       "      <th>num_dependents</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>account_id</th>\n",
       "      <th>tenure_months</th>\n",
       "      <th>num_referrals</th>\n",
       "      <th>...</th>\n",
       "      <th>total_refunds</th>\n",
       "      <th>status</th>\n",
       "      <th>churn_label</th>\n",
       "      <th>churn_category</th>\n",
       "      <th>churn_reason</th>\n",
       "      <th>area_id</th>\n",
       "      <th>city</th>\n",
       "      <th>latitutde</th>\n",
       "      <th>longitude</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002-ORFBO</td>\n",
       "      <td>Female</td>\n",
       "      <td>37</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>93225</td>\n",
       "      <td>XSWV-PAYXZ</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Stayed</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>649</td>\n",
       "      <td>Frazier Park</td>\n",
       "      <td>34.827662</td>\n",
       "      <td>-118.999073</td>\n",
       "      <td>4498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5183-SNMJQ</td>\n",
       "      <td>Male</td>\n",
       "      <td>32</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>93225</td>\n",
       "      <td>DZQJ-ZMREB</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>43.25</td>\n",
       "      <td>Stayed</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>649</td>\n",
       "      <td>Frazier Park</td>\n",
       "      <td>34.827662</td>\n",
       "      <td>-118.999073</td>\n",
       "      <td>4498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6847-KJLTS</td>\n",
       "      <td>Female</td>\n",
       "      <td>72</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>93225</td>\n",
       "      <td>VLBU-IQLTI</td>\n",
       "      <td>58</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Stayed</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>649</td>\n",
       "      <td>Frazier Park</td>\n",
       "      <td>34.827662</td>\n",
       "      <td>-118.999073</td>\n",
       "      <td>4498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8788-DOXSU</td>\n",
       "      <td>Male</td>\n",
       "      <td>46</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>93225</td>\n",
       "      <td>HSKL-QCEUU</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Stayed</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>649</td>\n",
       "      <td>Frazier Park</td>\n",
       "      <td>34.827662</td>\n",
       "      <td>-118.999073</td>\n",
       "      <td>4498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0003-MKNFE</td>\n",
       "      <td>Male</td>\n",
       "      <td>46</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>91206</td>\n",
       "      <td>VFUN-NFDPJ</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>38.33</td>\n",
       "      <td>Stayed</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>184</td>\n",
       "      <td>Glendale</td>\n",
       "      <td>34.162515</td>\n",
       "      <td>-118.203869</td>\n",
       "      <td>31297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id  gender  age senior_citizen married  num_dependents  zip_code   \n",
       "0  0002-ORFBO  Female   37             No     Yes               0     93225  \\\n",
       "1  5183-SNMJQ    Male   32             No      No               0     93225   \n",
       "2  6847-KJLTS  Female   72            Yes     Yes               0     93225   \n",
       "3  8788-DOXSU    Male   46             No      No               0     93225   \n",
       "4  0003-MKNFE    Male   46             No      No               0     91206   \n",
       "\n",
       "   account_id  tenure_months  num_referrals  ... total_refunds  status   \n",
       "0  XSWV-PAYXZ              9              2  ...          0.00  Stayed  \\\n",
       "1  DZQJ-ZMREB             10              0  ...         43.25  Stayed   \n",
       "2  VLBU-IQLTI             58              8  ...          0.00  Stayed   \n",
       "3  HSKL-QCEUU             59              0  ...          0.00  Stayed   \n",
       "4  VFUN-NFDPJ              9              0  ...         38.33  Stayed   \n",
       "\n",
       "  churn_label churn_category churn_reason area_id          city  latitutde   \n",
       "0          No            NaN          NaN     649  Frazier Park  34.827662  \\\n",
       "1          No            NaN          NaN     649  Frazier Park  34.827662   \n",
       "2          No            NaN          NaN     649  Frazier Park  34.827662   \n",
       "3          No            NaN          NaN     649  Frazier Park  34.827662   \n",
       "4          No            NaN          NaN     184      Glendale  34.162515   \n",
       "\n",
       "    longitude population  \n",
       "0 -118.999073       4498  \n",
       "1 -118.999073       4498  \n",
       "2 -118.999073       4498  \n",
       "3 -118.999073       4498  \n",
       "4 -118.203869      31297  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import BinaryEncoder\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\n",
    "def convert_to_categorical(df:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts the following columns to categorical data\n",
    "    \n",
    "    Yes No to 1/0\n",
    "    - has_internet_service\n",
    "    - has_phone_service\n",
    "    - has_unlimited_data\n",
    "    - has_multiple_lines\n",
    "    - has_premium_tech_support\n",
    "    - has_online_security\n",
    "    - has_online_backup\n",
    "    - has_device_protection\n",
    "    - paperless_billing\n",
    "    - stream_movie\n",
    "    - stream_music\n",
    "    - stream_tv\n",
    "    - churn_label\n",
    "    - senior_citizen\n",
    "    - married\n",
    "    - gender (1 for male, 0 for female)\n",
    "\n",
    "    One-hot encodes the following columns\n",
    "    - payment_method\n",
    "    - internet_type\n",
    "\n",
    "    Ordinal encodes the following columns\n",
    "    - contract_type (one-year, month-to-month, two-year) => (1,0,2) \n",
    "\n",
    "    Label Encodes the following columns\n",
    "    - status\n",
    "    \n",
    "    Binary encodes\n",
    "    - churn_category\n",
    "\n",
    "    \"\"\"\n",
    "    # Creating output_df\n",
    "    output_df = df.copy()\n",
    "\n",
    "    # Copying over account_id, customer_id\n",
    "    output_df['account_id'] = df['account_id']\n",
    "    output_df['customer_id'] = df['customer_id']\n",
    "    output_df['zip_code'] = df['zip_code']\n",
    "\n",
    "     # Converting yes/no to 1/0\n",
    "    mapping = {'Yes':1, 'No':0, \"Male\":1, \"Female\":0}\n",
    "    output_df['has_internet_service'] = df['has_internet_service'].map(mapping)\n",
    "    output_df['has_phone_service'] = df['has_phone_service'].map(mapping)\n",
    "    output_df['has_unlimited_data'] = df['has_unlimited_data'].map(mapping)\n",
    "    output_df['has_multiple_lines'] = df['has_multiple_lines'].map(mapping)\n",
    "    output_df['has_premium_tech_support'] = df['has_premium_tech_support'].map(mapping)\n",
    "    output_df['has_online_security'] = df['has_online_security'].map(mapping)\n",
    "    output_df['has_online_backup'] = df['has_online_backup'].map(mapping)\n",
    "    output_df['has_device_protection'] = df['has_device_protection'].map(mapping)\n",
    "    output_df['paperless_billing'] = df['paperless_billing'].map(mapping)\n",
    "    output_df['stream_movie'] = df['stream_movie'].map(mapping)\n",
    "    output_df['stream_music'] = df['stream_music'].map(mapping)\n",
    "    output_df['stream_tv'] = df['stream_tv'].map(mapping)\n",
    "    output_df['senior_citizen'] = df['senior_citizen'].map(mapping)\n",
    "    output_df['married'] = df['married'].map(mapping)\n",
    "    output_df['gender'] = df['gender'].map(mapping)\n",
    "    # Some are missing churn_labels, but have status as \"churned\"\n",
    "    output_df['churn_label'] = df['churn_label']\n",
    "    output_df.loc[df['status'] == 'Churned', 'churn_label'] = 1\n",
    "    output_df['churn_label'] = output_df['churn_label'].fillna(0)\n",
    "\n",
    "    output_df['churn_label'] = output_df['churn_label'].map(mapping)\n",
    "\n",
    "    # Ordinal Encoding\n",
    "    mapping = {'Month-to-Month':0, 'One Year':1, 'Two Year':2}\n",
    "    output_df['contract_type'] = df['contract_type'].map(mapping)\n",
    "\n",
    "    # One-hot encoding\n",
    "    one_hot_encoder = OneHotEncoder()\n",
    "    one_hot_encoded_cols = [\"payment_method\", \"internet_type\"]\n",
    "    encoded_features = one_hot_encoder.fit_transform(df[one_hot_encoded_cols])\n",
    "    encoded_df = pd.DataFrame(encoded_features.toarray(), columns=one_hot_encoder.get_feature_names_out(one_hot_encoded_cols))\n",
    "    output_df = pd.concat([output_df, encoded_df], axis=1)\n",
    "\n",
    "    # Label Encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    output_df['status'] = label_encoder.fit_transform(df['status'])\n",
    "\n",
    "    # Binary Encoding\n",
    "    binary_encoder = BinaryEncoder(cols=['churn_category'])\n",
    "    binary_encoder.fit_transform(df['churn_category'])\n",
    "    churn_cat = binary_encoder.transform(df['churn_category'])\n",
    "    output_df = pd.concat([output_df, churn_cat], axis=1)\n",
    "\n",
    "    # Copying over the numerical values\n",
    "    output_df['tenure_months'] = df['tenure_months']\n",
    "    output_df['avg_long_distance_fee_monthly'] = df[['avg_long_distance_fee_monthly']]\n",
    "    output_df['total_long_distance_fee'] = df[['total_long_distance_fee']]\n",
    "    output_df['avg_gb_download_monthly'] = df[['avg_gb_download_monthly']]\n",
    "    output_df['total_monthly_fee'] = df[['total_monthly_fee']]\n",
    "    output_df['total_charges_quarter'] = df[['total_charges_quarter']]\n",
    "    output_df['total_refunds'] = df[['total_refunds']]\n",
    "    output_df['population'] = df[['population']]\n",
    "\n",
    "    # Dropping tables\n",
    "    output_df = output_df.drop(columns=['internet_type', 'payment_method', 'churn_category', 'churn_reason'])\n",
    "\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_numerical_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, MinMaxScaler]:\n",
    "    \"\"\"\n",
    "    Scales all numerical data\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    numerical_cols = ['age', 'tenure_months', 'avg_long_distance_fee_monthly', 'total_long_distance_fee', 'avg_gb_download_monthly', 'total_monthly_fee', 'total_charges_quarter', 'total_refunds', 'population']\n",
    "    df.loc[:, numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "    return df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'customer_id': 0,\n",
       " 'gender': 0,\n",
       " 'age': 0,\n",
       " 'senior_citizen': 0,\n",
       " 'married': 0,\n",
       " 'num_dependents': 0,\n",
       " 'zip_code': 0,\n",
       " 'account_id': 0,\n",
       " 'tenure_months': 0,\n",
       " 'num_referrals': 0,\n",
       " 'has_internet_service': 0,\n",
       " 'has_unlimited_data': 0,\n",
       " 'has_phone_service': 0,\n",
       " 'has_multiple_lines': 0,\n",
       " 'has_premium_tech_support': 0,\n",
       " 'has_online_security': 0,\n",
       " 'has_online_backup': 0,\n",
       " 'has_device_protection': 0,\n",
       " 'contract_type': 0,\n",
       " 'paperless_billing': 0,\n",
       " 'avg_long_distance_fee_monthly': 0,\n",
       " 'total_long_distance_fee': 0,\n",
       " 'avg_gb_download_monthly': 0,\n",
       " 'stream_tv': 0,\n",
       " 'stream_movie': 0,\n",
       " 'stream_music': 0,\n",
       " 'total_monthly_fee': 0,\n",
       " 'total_charges_quarter': 0,\n",
       " 'total_refunds': 0,\n",
       " 'status': 0,\n",
       " 'churn_label': 1869,\n",
       " 'area_id': 0,\n",
       " 'city': 0,\n",
       " 'latitutde': 0,\n",
       " 'longitude': 0,\n",
       " 'population': 0,\n",
       " 'payment_method_Bank Withdrawal': 0,\n",
       " 'payment_method_Credit Card': 0,\n",
       " 'payment_method_Mailed Check': 0,\n",
       " 'internet_type_Cable': 0,\n",
       " 'internet_type_DSL': 0,\n",
       " 'internet_type_Fiber Optic': 0,\n",
       " 'internet_type_nan': 0,\n",
       " 'churn_category_0': 0,\n",
       " 'churn_category_1': 0,\n",
       " 'churn_category_2': 0}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing DF, neglecting scaling for me\n",
    "parsed_df = convert_to_categorical(DF)\n",
    "parsed_df, scaler = scale_numerical_data(parsed_df)\n",
    "parsed_df.head(1)\n",
    "get_NaN_count(parsed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>churn_label</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7030</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7031</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7032</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7035</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7041</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1869 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      churn_label  status\n",
       "5             NaN       0\n",
       "6             NaN       0\n",
       "7             NaN       0\n",
       "9             NaN       0\n",
       "12            NaN       0\n",
       "...           ...     ...\n",
       "7030          NaN       0\n",
       "7031          NaN       0\n",
       "7032          NaN       0\n",
       "7035          NaN       0\n",
       "7041          NaN       0\n",
       "\n",
       "[1869 rows x 2 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_df[parsed_df['churn_label'].isna()][['churn_label','status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def perform_pca(df: pd.DataFrame, n_components: int):\n",
    "    # Separate the features from the target variable (if applicable)\n",
    "    features = df.drop(columns=['status', 'customer_id', 'account_id', 'zip_code', 'city'])  # Replace 'target' with your status variable column name\n",
    "\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    transformed_features = pca.fit_transform(features)\n",
    "\n",
    "    # Create a new DataFrame with the transformed features\n",
    "    transformed_df = pd.DataFrame(data=transformed_features, columns=[f\"PC{i+1}\" for i in range(n_components)])\n",
    "\n",
    "    # Concatenate the transformed features with the status variable (if applicable)\n",
    "    if 'status' in df.columns:\n",
    "        transformed_df['status'] = df['status']\n",
    "\n",
    "    return transformed_df, pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>churn_label</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7030</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7031</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7032</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7035</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7041</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1869 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      churn_label  status\n",
       "5             NaN       0\n",
       "6             NaN       0\n",
       "7             NaN       0\n",
       "9             NaN       0\n",
       "12            NaN       0\n",
       "...           ...     ...\n",
       "7030          NaN       0\n",
       "7031          NaN       0\n",
       "7032          NaN       0\n",
       "7035          NaN       0\n",
       "7041          NaN       0\n",
       "\n",
       "[1869 rows x 2 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_df[parsed_df['churn_label'].isna()][['churn_label','status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m transformed_df, pca \u001b[39m=\u001b[39m perform_pca(parsed_df, \u001b[39m5\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m transformed_df\u001b[39m.\u001b[39mhead()\n",
      "Cell \u001b[1;32mIn[75], line 9\u001b[0m, in \u001b[0;36mperform_pca\u001b[1;34m(df, n_components)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39m# Perform PCA\u001b[39;00m\n\u001b[0;32m      8\u001b[0m pca \u001b[39m=\u001b[39m PCA(n_components\u001b[39m=\u001b[39mn_components)\n\u001b[1;32m----> 9\u001b[0m transformed_features \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39;49mfit_transform(features)\n\u001b[0;32m     11\u001b[0m \u001b[39m# Create a new DataFrame with the transformed features\u001b[39;00m\n\u001b[0;32m     12\u001b[0m transformed_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data\u001b[39m=\u001b[39mtransformed_features, columns\u001b[39m=\u001b[39m[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPC\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_components)])\n",
      "File \u001b[1;32mc:\\Users\\limxu\\pyenvs\\ai300\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\limxu\\pyenvs\\ai300\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:462\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \n\u001b[0;32m    441\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39mC-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m--> 462\u001b[0m U, S, Vt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X)\n\u001b[0;32m    463\u001b[0m U \u001b[39m=\u001b[39m U[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components_]\n\u001b[0;32m    465\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwhiten:\n\u001b[0;32m    466\u001b[0m     \u001b[39m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\limxu\\pyenvs\\ai300\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:485\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[39mif\u001b[39;00m issparse(X):\n\u001b[0;32m    480\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPCA does not support sparse input. See \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTruncatedSVD for a possible alternative.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    483\u001b[0m     )\n\u001b[1;32m--> 485\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    486\u001b[0m     X, dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32], ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy\n\u001b[0;32m    487\u001b[0m )\n\u001b[0;32m    489\u001b[0m \u001b[39m# Handle n_components==None\u001b[39;00m\n\u001b[0;32m    490\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\limxu\\pyenvs\\ai300\\lib\\site-packages\\sklearn\\base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 565\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    567\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Users\\limxu\\pyenvs\\ai300\\lib\\site-packages\\sklearn\\utils\\validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    916\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    917\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    918\u001b[0m         )\n\u001b[0;32m    920\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 921\u001b[0m         _assert_all_finite(\n\u001b[0;32m    922\u001b[0m             array,\n\u001b[0;32m    923\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    924\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    925\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    926\u001b[0m         )\n\u001b[0;32m    928\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    929\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Users\\limxu\\pyenvs\\ai300\\lib\\site-packages\\sklearn\\utils\\validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    145\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    148\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m     )\n\u001b[1;32m--> 161\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "transformed_df, pca = perform_pca(parsed_df, 5)\n",
    "transformed_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance\n",
    "Not churned = 5174 ~ 75%\n",
    "\n",
    "Churned = 1817 ~25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "churn_label\n",
      "0.0    5174\n",
      "1.0    1817\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking for class imbalance\n",
    "print(parsed_df['churn_label'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_zscore(df:pd.DataFrame, threshold:int=3)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Detect outliers using the z-score method.\n",
    "    Returns a boolean mask indicating the outliers.\n",
    "    \"\"\"\n",
    "    z_scores = (df - np.mean(df)) / np.std(df)\n",
    "    outliers = np.abs(z_scores) > threshold\n",
    "    return outliers\n",
    "\n",
    "def plot_boxplot(df_column:pd.Series)->None:\n",
    "    \"\"\"\n",
    "    Plots a box plot based on the given DataFrame column using Plotly.\n",
    "    \"\"\"\n",
    "    # Plotting boxplot using Plotly\n",
    "    fig = px.box(df_column, title='Box Plot')\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "\n",
    "def get_value_at_percentile(df:pd.DataFrame, column:str, percentile:int)->float:\n",
    "    \"\"\"\n",
    "    Returns the value at a specific percentile in a DataFrame column.\n",
    "    \"\"\"\n",
    "    # Get the values from the specified column\n",
    "    column_values = df[column].values\n",
    "\n",
    "    # Calculate the percentile value\n",
    "    value_at_percentile = np.percentile(column_values, percentile)\n",
    "\n",
    "    return value_at_percentile\n",
    "\n",
    "def remove_rows_by_values(df:pd.DataFrame, column:str, lower_bound:float, upper_bound:float):\n",
    "    \"\"\"\n",
    "    Removes rows from a DataFrame based on specific column values.\n",
    "    \"\"\"\n",
    "    # Create a boolean mask to filter rows with the specified column values\n",
    "    mask = (df[column] >= lower_bound) & (df[column] <= upper_bound)\n",
    "\n",
    "    # Filter the DataFrame using the boolean mask\n",
    "    updated_df = df[mask]\n",
    "\n",
    "    return updated_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Analysis Summary\n",
    "### ACC_DF\n",
    "\n",
    "1. tenure_months\n",
    "\n",
    "In the dataset, 362 rows have tenure_months at 72 while 0 rows have tenure_months at 71. As 72 months is 3 years, this suggests that the maximum contract period is 3 years. 362 rows consists of approximately 5% of the entire data set we have currently, hence I will not be removing any outliers based on tenure_months.\n",
    "\n",
    "2. num_referrals\n",
    "\n",
    "I decide to keep the 0th to 95th percentile of the rows based on the number of referrals. This meant the cutoff value would be 9, and 225 rows will be removed as a results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACC_DF = pd.read_csv(ACC_PATH)\n",
    "# ACC_DF.head()\n",
    "# # Keeping from 80th percentile to 0th percentile for tenure_months\n",
    "# # plot_boxplot(ACC_DF['num_referrals'])\n",
    "# get_value_at_percentile(ACC_DF, 'num_referrals', 80)\n",
    "\n",
    "# def acc_df_remove_outliers(df:pd.DataFrame):\n",
    "#     # Tenure_months: Keep from 0th to 80th percentile\n",
    "#     # Num_referrals: Keep from 0th to 80th percentile\n",
    "#     tenure_months_upper_bound = get_value_at_percentile(df, 'tenure_months', 90)\n",
    "#     num_referarals_upper_bound = get_value_at_percentile(df, 'num_referrals', 80)\n",
    "\n",
    "#     # Removing outliers\n",
    "#     output_df = remove_rows_by_values(df, 'tenure_months', 0, tenure_months_upper_bound)\n",
    "#     print(f\"Removed {len(df) - len(output_df)} rows from the DataFrame based on the 'tenure_months' column values.\")\n",
    "\n",
    "#     output_df = remove_rows_by_values(df, 'num_referrals', 0, num_referarals_upper_bound)\n",
    "#     print(f\"Removed {len(df) - len(output_df)} rows from the DataFrame based on the 'num_referrals' column values.\")\n",
    "\n",
    "#     return output_df\n",
    "\n",
    "# plot_boxplot(ACC_DF['tenure_months'])\n",
    "# cleaned_acc_df = acc_df_remove_outliers(ACC_DF)\n",
    "# plot_boxplot(cleaned_acc_df['tenure_months'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_values(df:pd.DataFrame)->dict:\n",
    "    \"\"\"\n",
    "    Runs through the columns of a dataframe and prints the unique values of each column. \n",
    "    \"\"\"\n",
    "    dict_unique_values = {}\n",
    "    for cols in df.columns:\n",
    "        dict_unique_values[cols] = df[cols].unique()\n",
    "    return dict_unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NaN_count(df:pd.DataFrame)->dict:\n",
    "    \"\"\"\n",
    "    Returns the number of NaN values for each column in a dictionary.\n",
    "    \"\"\"\n",
    "    nan_count = df.isna().sum().to_dict()\n",
    "    return nan_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_acc_df(df:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocesses the account dataframe as below:\n",
    "\n",
    "    Converts follow columns (yes/no) to 1/0\n",
    "    - has_internet_service\n",
    "    - has_phone_service\n",
    "    - has_unlimited_data\n",
    "    - has_multiple_lines\n",
    "    - has_premium_tech_support\n",
    "    - has_online_security\n",
    "    - has_online_backup\n",
    "    - has_device_protection\n",
    "    - paperless_billing\n",
    "\n",
    "    Converts follow columns to ordinal values:\n",
    "    - contract_type (one-year, month-to-month, two-year) => (1,0,2) \n",
    "\n",
    "    One-hot encodes following columns:\n",
    "    - payment_method\n",
    "    - internet_type\n",
    "\n",
    "    Scales following columns:\n",
    "    - tenure_months\n",
    "    \"\"\"\n",
    "    # Creating a new df\n",
    "    output_df = pd.DataFrame()\n",
    "\n",
    "    # Copying over account_id\tcustomer_id\ttenure_months\n",
    "    output_df['account_id'] = df['account_id']\n",
    "    output_df['customer_id'] = df['customer_id']\n",
    "\n",
    "    # Converting yes/no to 1/0\n",
    "    mapping = {'Yes':1, 'No':0}\n",
    "\n",
    "    output_df['has_internet_service'] = df['has_internet_service'].map(mapping)\n",
    "    output_df['has_phone_service'] = df['has_phone_service'].map(mapping)\n",
    "    output_df['has_unlimited_data'] = df['has_unlimited_data'].map(mapping)\n",
    "    output_df['has_multiple_lines'] = df['has_multiple_lines'].map(mapping)\n",
    "    output_df['has_premium_tech_support'] = df['has_premium_tech_support'].map(mapping)\n",
    "    output_df['has_online_security'] = df['has_online_security'].map(mapping)\n",
    "    output_df['has_online_backup'] = df['has_online_backup'].map(mapping)\n",
    "    output_df['has_device_protection'] = df['has_device_protection'].map(mapping)\n",
    "    output_df['paperless_billing'] = df['paperless_billing'].map(mapping)\n",
    "\n",
    "    mapping = {'Month-to-Month':0, 'One Year':1, 'Two Year':2}\n",
    "    output_df['contract_type'] = df['contract_type'].map(mapping)\n",
    "\n",
    "    # One-hot encoding\n",
    "    # one_hot_encoder = OneHotEncoder()\n",
    "    # one_hot_encoded_cols = [\"payment_method\", \"internet_type\"]\n",
    "    # encoded_df = pd.DataFrame(one_hot_encoder.fit_transform(df[one_hot_encoded_cols]))\n",
    "    # encoded_df.columns = one_hot_encoder.get_feature_names_out(one_hot_encoded_cols)\n",
    "    # print(encoded_df.head())\n",
    "\n",
    "    # One-hot encoding\n",
    "    one_hot_encoder = OneHotEncoder()\n",
    "    one_hot_encoded_cols = [\"payment_method\", \"internet_type\"]\n",
    "    encoded_features = one_hot_encoder.fit_transform(df[one_hot_encoded_cols])\n",
    "    encoded_df = pd.DataFrame(encoded_features.toarray(), columns=one_hot_encoder.get_feature_names_out(one_hot_encoded_cols))\n",
    "    output_df = pd.concat([output_df, encoded_df], axis=1)\n",
    "\n",
    "    # Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    output_df['tenure_months'] = scaler.fit_transform(df[['tenure_months']])\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Account Usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_acc_usage_df(df:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the account usage dataframe as below:\n",
    "    Scale the following columns:\n",
    "    - avg_long_distance_fee_monthly\n",
    "    - total_long_distance_fee\n",
    "    - avg_gb_download_monthly\n",
    "    - total_monthly_fee\n",
    "    - total_chargers_quarter\n",
    "    - total_refunds\n",
    "    Converts following col to 1/0:\n",
    "    - stream_move\n",
    "    - stream_music\n",
    "    - stream_tv\n",
    "    \"\"\"\n",
    "    # Create new df\n",
    "    output_df = pd.DataFrame()\n",
    "\n",
    "    # Copying over acc_id\n",
    "    output_df['account_id'] = df['account_id']\n",
    "\n",
    "    # Converting yes/no to 1/0\n",
    "    mapping = {'Yes':1, 'No':0}\n",
    "\n",
    "    output_df['stream_movie'] = df['stream_movie'].map(mapping)\n",
    "    output_df['stream_music'] = df['stream_music'].map(mapping)\n",
    "    output_df['stream_tv'] = df['stream_tv'].map(mapping)\n",
    "\n",
    "    # Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    output_df['avg_long_distance_fee_monthly'] = scaler.fit_transform(df[['avg_long_distance_fee_monthly']])\n",
    "    output_df['total_long_distance_fee'] = scaler.fit_transform(df[['total_long_distance_fee']])\n",
    "    output_df['avg_gb_download_monthly'] = scaler.fit_transform(df[['avg_gb_download_monthly']])\n",
    "    output_df['total_monthly_fee'] = scaler.fit_transform(df[['total_monthly_fee']])\n",
    "    output_df['total_charges_quarter'] = scaler.fit_transform(df[['total_charges_quarter']])\n",
    "    output_df['total_refunds'] = scaler.fit_transform(df[['total_refunds']])\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Churn Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocess_churn_df(df:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the churn status dataframe as below:\n",
    "    Convert following col to 1/0:\n",
    "    - churn_label\n",
    "\n",
    "    Binary encodes:\n",
    "    - churn_category\n",
    "    \n",
    "    Label-encodes:\n",
    "    - status\n",
    "\n",
    "    Drops the following:\n",
    "    - churn_reason -> Not planning to do NLP\n",
    "    \"\"\"\n",
    "    # Create new df\n",
    "    output_df = pd.DataFrame()\n",
    "\n",
    "    # Copying over acc_id\n",
    "    output_df['customer_id'] = df['customer_id']\n",
    "\n",
    "    # Converting yes/no to 1/0\n",
    "    # Additional step to check status \n",
    "    output_df['churn_label'] = df['churn_label'].fillna(0)\n",
    "    output_df.loc[df['status'] == 'Churned', 'churn_label'] = 1\n",
    "    mapping = {'Yes':1, 'No':0, }\n",
    "    output_df['churn_label'] = df['churn_label'].map(mapping)\n",
    "\n",
    "    # Binary Encoding\n",
    "    binary_encoder = BinaryEncoder(cols=['churn_category'])\n",
    "    binary_encoder.fit_transform(df['churn_category'])\n",
    "    churn_cat = binary_encoder.transform(df['churn_category'])\n",
    "    output_df = pd.concat([output_df, churn_cat], axis=1)\n",
    "\n",
    "    # Label Encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    output_df['status'] = label_encoder.fit_transform(df['status'])\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_city_df(df:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the city dataframe as below:\n",
    "    Min-max scales population column\n",
    "    Keep area_id and population only\n",
    "    \"\"\"\n",
    "    # Creating new df\n",
    "    output_df = pd.DataFrame()\n",
    "\n",
    "    # Copying over area_id\n",
    "    output_df['zip_code'] = df['zip_code']\n",
    "\n",
    "    # Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    output_df['population'] = scaler.fit_transform(df[['population']])\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_customer_df(df:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the customer dataframe as below:\n",
    "    Convert following col to 1/0:\n",
    "    - senior_citizen\n",
    "    - married\n",
    "    - gender (1 for male, 0 for female)\n",
    "    \"\"\"\n",
    "    # Create new df\n",
    "    output_df = pd.DataFrame()\n",
    "\n",
    "    # Copying over customer_id, zip_code\n",
    "    output_df['customer_id'] = df['customer_id']\n",
    "    output_df['zip_code'] = df['zip_code']\n",
    "\n",
    "    # Converting columns to 1/0\n",
    "    mapping = {'Yes':1, 'No':0, \"Male\":1, \"Female\":0}\n",
    "    output_df['senior_citizen'] = df['senior_citizen'].map(mapping)\n",
    "    output_df['married'] = df['married'].map(mapping)\n",
    "    output_df['gender'] = df['gender'].map(mapping)\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing all CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dfs from CSV\n",
    "ACCOUNT_DF = pd.read_csv(ACC_PATH)\n",
    "ACCOUNT_USAGE_DF = pd.read_csv(ACC_USAGE_PATH)\n",
    "CHURN_STATUS_DF = pd.read_csv(CHURN_STATUS_PATH)\n",
    "CITY_DF = pd.read_csv(CITY_PATH)\n",
    "CUSTOMER_DF = pd.read_csv(CUSTOMER_PATH)\n",
    "\n",
    "# Preprocessing dfs\n",
    "account_df = preprocess_acc_df(ACCOUNT_DF)\n",
    "account_usage_df = preprocess_acc_usage_df(ACCOUNT_USAGE_DF)\n",
    "churn_status_df = preprocess_churn_df(CHURN_STATUS_DF)\n",
    "city_df = preprocess_city_df(CITY_DF)\n",
    "customer_df = preprocess_customer_df(CUSTOMER_DF)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining customer and account tables on customer_id\n",
    "df = pd.merge(customer_df, account_df, on='customer_id', how='inner')\n",
    "# Joining df with account_usage_df on account_id\n",
    "df = pd.merge(df, account_usage_df, on='account_id', how='inner')\n",
    "# Joining df with churn_status_df on customer_id\n",
    "df = pd.merge(df, churn_status_df, on='customer_id', how='inner')\n",
    "# Joining df with city_df on area_id\n",
    "df = pd.merge(df, city_df, on='zip_code', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai300",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
