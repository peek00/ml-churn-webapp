{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting and EDA for Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "from dotenv import dotenv_values\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting env var\n",
    "secrets = dotenv_values()\n",
    "\n",
    "host =secrets['DB_HOST']\n",
    "name = secrets['DB_NAME']\n",
    "user = secrets['DB_USER']\n",
    "pwd = secrets['DB_PWD']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7043\n"
     ]
    }
   ],
   "source": [
    "# Connecting to DB\n",
    "\n",
    "connection = pymysql.connect(\n",
    "    host = host,\n",
    "    user = user,\n",
    "    password = pwd,\n",
    "    database = name\n",
    "    )\n",
    "\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"SELECT * FROM churn_status\")\n",
    "\n",
    "results = cursor.fetchall()\n",
    "\n",
    "print(len(results))\n",
    "\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "Done from local data files\n",
    "\n",
    "---\n",
    "\n",
    "To Do List\n",
    "- Join unscaled and unprocessed datasets. \n",
    "- Remove rows without churn label (if any)\n",
    "- Check for class imbalance -> if have, perform stratified split \n",
    "- Split into train-validation (70-30)\n",
    "- Perform outlier removal only on train set\n",
    "- After outlier removal, preprocess (categorical encode and scale) columns in both datasets\n",
    "- Perform PCA to determine which features to use\n",
    "- Train machine learning model with kfold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "ACC_PATH = \"data_given/1_account.csv\"\n",
    "ACC_USAGE_PATH = \"data_given/2_account_usage.csv\"\n",
    "CHURN_STATUS_PATH = \"data_given/3_churn_status.csv\"\n",
    "CITY_PATH = \"data_given/4_city.csv\"\n",
    "CUSTOMER_PATH = \"data_given/5_customer.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining all unprocessed tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading DFs\n",
    "CUSTOMER_DF = pd.read_csv(CUSTOMER_PATH)\n",
    "ACCOUNT_DF = pd.read_csv(ACC_PATH)\n",
    "ACC_USAGE_DF = pd.read_csv(ACC_USAGE_PATH)\n",
    "CHURN_STATUS_DF = pd.read_csv(CHURN_STATUS_PATH)\n",
    "CITY_DF = pd.read_csv(CITY_PATH)\n",
    "\n",
    "# Joining customer and account tables on customer_id\n",
    "DF = pd.merge(CUSTOMER_DF, ACCOUNT_DF, on='customer_id', how='inner')\n",
    "# Joining df with account_usage_df on account_id\n",
    "DF = pd.merge(DF, ACC_USAGE_DF, on='account_id', how='inner')\n",
    "# Joining df with churn_status_df on customer_id\n",
    "DF = pd.merge(DF, CHURN_STATUS_DF, on='customer_id', how='inner')\n",
    "# Joining df with city_df on area_id\n",
    "DF = pd.merge(DF, CITY_DF, on='zip_code', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>senior_citizen</th>\n",
       "      <th>married</th>\n",
       "      <th>num_dependents</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>account_id</th>\n",
       "      <th>tenure_months</th>\n",
       "      <th>num_referrals</th>\n",
       "      <th>...</th>\n",
       "      <th>total_refunds</th>\n",
       "      <th>status</th>\n",
       "      <th>churn_label</th>\n",
       "      <th>churn_category</th>\n",
       "      <th>churn_reason</th>\n",
       "      <th>area_id</th>\n",
       "      <th>city</th>\n",
       "      <th>latitutde</th>\n",
       "      <th>longitude</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002-ORFBO</td>\n",
       "      <td>Female</td>\n",
       "      <td>37</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>93225</td>\n",
       "      <td>XSWV-PAYXZ</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Stayed</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>649</td>\n",
       "      <td>Frazier Park</td>\n",
       "      <td>34.827662</td>\n",
       "      <td>-118.999073</td>\n",
       "      <td>4498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5183-SNMJQ</td>\n",
       "      <td>Male</td>\n",
       "      <td>32</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>93225</td>\n",
       "      <td>DZQJ-ZMREB</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>43.25</td>\n",
       "      <td>Stayed</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>649</td>\n",
       "      <td>Frazier Park</td>\n",
       "      <td>34.827662</td>\n",
       "      <td>-118.999073</td>\n",
       "      <td>4498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6847-KJLTS</td>\n",
       "      <td>Female</td>\n",
       "      <td>72</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>93225</td>\n",
       "      <td>VLBU-IQLTI</td>\n",
       "      <td>58</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Stayed</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>649</td>\n",
       "      <td>Frazier Park</td>\n",
       "      <td>34.827662</td>\n",
       "      <td>-118.999073</td>\n",
       "      <td>4498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8788-DOXSU</td>\n",
       "      <td>Male</td>\n",
       "      <td>46</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>93225</td>\n",
       "      <td>HSKL-QCEUU</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Stayed</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>649</td>\n",
       "      <td>Frazier Park</td>\n",
       "      <td>34.827662</td>\n",
       "      <td>-118.999073</td>\n",
       "      <td>4498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0003-MKNFE</td>\n",
       "      <td>Male</td>\n",
       "      <td>46</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>91206</td>\n",
       "      <td>VFUN-NFDPJ</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>38.33</td>\n",
       "      <td>Stayed</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>184</td>\n",
       "      <td>Glendale</td>\n",
       "      <td>34.162515</td>\n",
       "      <td>-118.203869</td>\n",
       "      <td>31297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id  gender  age senior_citizen married  num_dependents  zip_code   \n",
       "0  0002-ORFBO  Female   37             No     Yes               0     93225  \\\n",
       "1  5183-SNMJQ    Male   32             No      No               0     93225   \n",
       "2  6847-KJLTS  Female   72            Yes     Yes               0     93225   \n",
       "3  8788-DOXSU    Male   46             No      No               0     93225   \n",
       "4  0003-MKNFE    Male   46             No      No               0     91206   \n",
       "\n",
       "   account_id  tenure_months  num_referrals  ... total_refunds  status   \n",
       "0  XSWV-PAYXZ              9              2  ...          0.00  Stayed  \\\n",
       "1  DZQJ-ZMREB             10              0  ...         43.25  Stayed   \n",
       "2  VLBU-IQLTI             58              8  ...          0.00  Stayed   \n",
       "3  HSKL-QCEUU             59              0  ...          0.00  Stayed   \n",
       "4  VFUN-NFDPJ              9              0  ...         38.33  Stayed   \n",
       "\n",
       "  churn_label churn_category churn_reason area_id          city  latitutde   \n",
       "0          No            NaN          NaN     649  Frazier Park  34.827662  \\\n",
       "1          No            NaN          NaN     649  Frazier Park  34.827662   \n",
       "2          No            NaN          NaN     649  Frazier Park  34.827662   \n",
       "3          No            NaN          NaN     649  Frazier Park  34.827662   \n",
       "4          No            NaN          NaN     184      Glendale  34.162515   \n",
       "\n",
       "    longitude population  \n",
       "0 -118.999073       4498  \n",
       "1 -118.999073       4498  \n",
       "2 -118.999073       4498  \n",
       "3 -118.999073       4498  \n",
       "4 -118.203869      31297  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_categorical(df:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts the following columns to categorical data\n",
    "    \n",
    "    Yes No to 1/0\n",
    "    - has_internet_service\n",
    "    - has_phone_service\n",
    "    - has_unlimited_data\n",
    "    - has_multiple_lines\n",
    "    - has_premium_tech_support\n",
    "    - has_online_security\n",
    "    - has_online_backup\n",
    "    - has_device_protection\n",
    "    - paperless_billing\n",
    "    - stream_movie\n",
    "    - stream_music\n",
    "    - stream_tv\n",
    "    - churn_label\n",
    "    - senior_citizen\n",
    "    - married\n",
    "    - gender (1 for male, 0 for female)\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    # Creating output_df\n",
    "    output_df = df.copy()\n",
    "\n",
    "     # Converting yes/no to 1/0\n",
    "    mapping = {'Yes':1, 'No':0}\n",
    "    output_df['has_internet_service'] = df['has_internet_service'].map(mapping)\n",
    "    output_df['has_phone_service'] = df['has_phone_service'].map(mapping)\n",
    "    output_df['has_unlimited_data'] = df['has_unlimited_data'].map(mapping)\n",
    "    output_df['has_multiple_lines'] = df['has_multiple_lines'].map(mapping)\n",
    "    output_df['has_premium_tech_support'] = df['has_premium_tech_support'].map(mapping)\n",
    "    output_df['has_online_security'] = df['has_online_security'].map(mapping)\n",
    "    output_df['has_online_backup'] = df['has_online_backup'].map(mapping)\n",
    "    output_df['has_device_protection'] = df['has_device_protection'].map(mapping)\n",
    "    output_df['paperless_billing'] = df['paperless_billing'].map(mapping)\n",
    "    output_df['stream_movie'] = df['stream_movie'].map(mapping)\n",
    "    output_df['stream_music'] = df['stream_music'].map(mapping)\n",
    "    output_df['stream_tv'] = df['stream_tv'].map(mapping)\n",
    "    output_df['senior_citizen'] = df['senior_citizen'].map(mapping)\n",
    "    output_df['married'] = df['married'].map(mapping)\n",
    "    output_df['gender'] = df['gender'].map(mapping)\n",
    "    # Some are missing churn_labels, but have status as \"churned\"\n",
    "    output_df['churn_label'] = df['churn_label'].fillna(0)\n",
    "    output_df.loc[df['status'] == 'Churned', 'churn_label'] = 1\n",
    "    mapping = {'Yes':1, 'No':0, }\n",
    "    output_df['churn_label'] = df['churn_label'].map(mapping)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_zscore(df:pd.DataFrame, threshold:int=3)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Detect outliers using the z-score method.\n",
    "    Returns a boolean mask indicating the outliers.\n",
    "    \"\"\"\n",
    "    z_scores = (df - np.mean(df)) / np.std(df)\n",
    "    outliers = np.abs(z_scores) > threshold\n",
    "    return outliers\n",
    "\n",
    "def plot_boxplot(df_column:pd.Series)->None:\n",
    "    \"\"\"\n",
    "    Plots a box plot based on the given DataFrame column using Plotly.\n",
    "    \"\"\"\n",
    "    # Plotting boxplot using Plotly\n",
    "    fig = px.box(df_column, title='Box Plot')\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "\n",
    "def get_value_at_percentile(df:pd.DataFrame, column:str, percentile:int)->float:\n",
    "    \"\"\"\n",
    "    Returns the value at a specific percentile in a DataFrame column.\n",
    "    \"\"\"\n",
    "    # Get the values from the specified column\n",
    "    column_values = df[column].values\n",
    "\n",
    "    # Calculate the percentile value\n",
    "    value_at_percentile = np.percentile(column_values, percentile)\n",
    "\n",
    "    return value_at_percentile\n",
    "\n",
    "def remove_rows_by_values(df:pd.DataFrame, column:str, lower_bound:float, upper_bound:float):\n",
    "    \"\"\"\n",
    "    Removes rows from a DataFrame based on specific column values.\n",
    "    \"\"\"\n",
    "    # Create a boolean mask to filter rows with the specified column values\n",
    "    mask = (df[column] >= lower_bound) & (df[column] <= upper_bound)\n",
    "\n",
    "    # Filter the DataFrame using the boolean mask\n",
    "    updated_df = df[mask]\n",
    "\n",
    "    return updated_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Analysis Summary\n",
    "### ACC_DF\n",
    "\n",
    "1. tenure_months\n",
    "\n",
    "In the dataset, 362 rows have tenure_months at 72 while 0 rows have tenure_months at 71. As 72 months is 3 years, this suggests that the maximum contract period is 3 years. 362 rows consists of approximately 5% of the entire data set we have currently, hence I will not be removing any outliers based on tenure_months.\n",
    "\n",
    "2. num_referrals\n",
    "\n",
    "I decide to keep the 0th to 95th percentile of the rows based on the number of referrals. This meant the cutoff value would be 9, and 225 rows will be removed as a results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACC_DF = pd.read_csv(ACC_PATH)\n",
    "# ACC_DF.head()\n",
    "# # Keeping from 80th percentile to 0th percentile for tenure_months\n",
    "# # plot_boxplot(ACC_DF['num_referrals'])\n",
    "# get_value_at_percentile(ACC_DF, 'num_referrals', 80)\n",
    "\n",
    "# def acc_df_remove_outliers(df:pd.DataFrame):\n",
    "#     # Tenure_months: Keep from 0th to 80th percentile\n",
    "#     # Num_referrals: Keep from 0th to 80th percentile\n",
    "#     tenure_months_upper_bound = get_value_at_percentile(df, 'tenure_months', 90)\n",
    "#     num_referarals_upper_bound = get_value_at_percentile(df, 'num_referrals', 80)\n",
    "\n",
    "#     # Removing outliers\n",
    "#     output_df = remove_rows_by_values(df, 'tenure_months', 0, tenure_months_upper_bound)\n",
    "#     print(f\"Removed {len(df) - len(output_df)} rows from the DataFrame based on the 'tenure_months' column values.\")\n",
    "\n",
    "#     output_df = remove_rows_by_values(df, 'num_referrals', 0, num_referarals_upper_bound)\n",
    "#     print(f\"Removed {len(df) - len(output_df)} rows from the DataFrame based on the 'num_referrals' column values.\")\n",
    "\n",
    "#     return output_df\n",
    "\n",
    "# plot_boxplot(ACC_DF['tenure_months'])\n",
    "# cleaned_acc_df = acc_df_remove_outliers(ACC_DF)\n",
    "# plot_boxplot(cleaned_acc_df['tenure_months'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_values(df:pd.DataFrame)->dict:\n",
    "    \"\"\"\n",
    "    Runs through the columns of a dataframe and prints the unique values of each column. \n",
    "    \"\"\"\n",
    "    dict_unique_values = {}\n",
    "    for cols in df.columns:\n",
    "        dict_unique_values[cols] = df[cols].unique()\n",
    "    return dict_unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NaN_count(df:pd.DataFrame)->dict:\n",
    "    \"\"\"\n",
    "    Returns the number of NaN values for each column in a dictionary.\n",
    "    \"\"\"\n",
    "    nan_count = df.isna().sum().to_dict()\n",
    "    return nan_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def preprocess_acc_df(df:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocesses the account dataframe as below:\n",
    "\n",
    "    Converts follow columns (yes/no) to 1/0\n",
    "    - has_internet_service\n",
    "    - has_phone_service\n",
    "    - has_unlimited_data\n",
    "    - has_multiple_lines\n",
    "    - has_premium_tech_support\n",
    "    - has_online_security\n",
    "    - has_online_backup\n",
    "    - has_device_protection\n",
    "    - paperless_billing\n",
    "\n",
    "    Converts follow columns to ordinal values:\n",
    "    - contract_type (one-year, month-to-month, two-year) => (1,0,2) \n",
    "\n",
    "    One-hot encodes following columns:\n",
    "    - payment_method\n",
    "    - internet_type\n",
    "\n",
    "    Scales following columns:\n",
    "    - tenure_months\n",
    "    \"\"\"\n",
    "    # Creating a new df\n",
    "    output_df = pd.DataFrame()\n",
    "\n",
    "    # Copying over account_id\tcustomer_id\ttenure_months\n",
    "    output_df['account_id'] = df['account_id']\n",
    "    output_df['customer_id'] = df['customer_id']\n",
    "\n",
    "    # Converting yes/no to 1/0\n",
    "    mapping = {'Yes':1, 'No':0}\n",
    "\n",
    "    output_df['has_internet_service'] = df['has_internet_service'].map(mapping)\n",
    "    output_df['has_phone_service'] = df['has_phone_service'].map(mapping)\n",
    "    output_df['has_unlimited_data'] = df['has_unlimited_data'].map(mapping)\n",
    "    output_df['has_multiple_lines'] = df['has_multiple_lines'].map(mapping)\n",
    "    output_df['has_premium_tech_support'] = df['has_premium_tech_support'].map(mapping)\n",
    "    output_df['has_online_security'] = df['has_online_security'].map(mapping)\n",
    "    output_df['has_online_backup'] = df['has_online_backup'].map(mapping)\n",
    "    output_df['has_device_protection'] = df['has_device_protection'].map(mapping)\n",
    "    output_df['paperless_billing'] = df['paperless_billing'].map(mapping)\n",
    "\n",
    "    mapping = {'Month-to-Month':0, 'One Year':1, 'Two Year':2}\n",
    "    output_df['contract_type'] = df['contract_type'].map(mapping)\n",
    "\n",
    "    # One-hot encoding\n",
    "    # one_hot_encoder = OneHotEncoder()\n",
    "    # one_hot_encoded_cols = [\"payment_method\", \"internet_type\"]\n",
    "    # encoded_df = pd.DataFrame(one_hot_encoder.fit_transform(df[one_hot_encoded_cols]))\n",
    "    # encoded_df.columns = one_hot_encoder.get_feature_names_out(one_hot_encoded_cols)\n",
    "    # print(encoded_df.head())\n",
    "\n",
    "    # One-hot encoding\n",
    "    one_hot_encoder = OneHotEncoder()\n",
    "    one_hot_encoded_cols = [\"payment_method\", \"internet_type\"]\n",
    "    encoded_features = one_hot_encoder.fit_transform(df[one_hot_encoded_cols])\n",
    "    encoded_df = pd.DataFrame(encoded_features.toarray(), columns=one_hot_encoder.get_feature_names_out(one_hot_encoded_cols))\n",
    "    output_df = pd.concat([output_df, encoded_df], axis=1)\n",
    "\n",
    "    # Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    output_df['tenure_months'] = scaler.fit_transform(df[['tenure_months']])\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Account Usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_acc_usage_df(df:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the account usage dataframe as below:\n",
    "    Scale the following columns:\n",
    "    - avg_long_distance_fee_monthly\n",
    "    - total_long_distance_fee\n",
    "    - avg_gb_download_monthly\n",
    "    - total_monthly_fee\n",
    "    - total_chargers_quarter\n",
    "    - total_refunds\n",
    "    Converts following col to 1/0:\n",
    "    - stream_move\n",
    "    - stream_music\n",
    "    - stream_tv\n",
    "    \"\"\"\n",
    "    # Create new df\n",
    "    output_df = pd.DataFrame()\n",
    "\n",
    "    # Copying over acc_id\n",
    "    output_df['account_id'] = df['account_id']\n",
    "\n",
    "    # Converting yes/no to 1/0\n",
    "    mapping = {'Yes':1, 'No':0}\n",
    "\n",
    "    output_df['stream_movie'] = df['stream_movie'].map(mapping)\n",
    "    output_df['stream_music'] = df['stream_music'].map(mapping)\n",
    "    output_df['stream_tv'] = df['stream_tv'].map(mapping)\n",
    "\n",
    "    # Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    output_df['avg_long_distance_fee_monthly'] = scaler.fit_transform(df[['avg_long_distance_fee_monthly']])\n",
    "    output_df['total_long_distance_fee'] = scaler.fit_transform(df[['total_long_distance_fee']])\n",
    "    output_df['avg_gb_download_monthly'] = scaler.fit_transform(df[['avg_gb_download_monthly']])\n",
    "    output_df['total_monthly_fee'] = scaler.fit_transform(df[['total_monthly_fee']])\n",
    "    output_df['total_charges_quarter'] = scaler.fit_transform(df[['total_charges_quarter']])\n",
    "    output_df['total_refunds'] = scaler.fit_transform(df[['total_refunds']])\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Churn Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import BinaryEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def preprocess_churn_df(df:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the churn status dataframe as below:\n",
    "    Convert following col to 1/0:\n",
    "    - churn_label\n",
    "\n",
    "    Binary encodes:\n",
    "    - churn_category\n",
    "    \n",
    "    Label-encodes:\n",
    "    - status\n",
    "\n",
    "    Drops the following:\n",
    "    - churn_reason -> Not planning to do NLP\n",
    "    \"\"\"\n",
    "    # Create new df\n",
    "    output_df = pd.DataFrame()\n",
    "\n",
    "    # Copying over acc_id\n",
    "    output_df['customer_id'] = df['customer_id']\n",
    "\n",
    "    # Converting yes/no to 1/0\n",
    "    # Additional step to check status \n",
    "    output_df['churn_label'] = df['churn_label'].fillna(0)\n",
    "    output_df.loc[df['status'] == 'Churned', 'churn_label'] = 1\n",
    "    mapping = {'Yes':1, 'No':0, }\n",
    "    output_df['churn_label'] = df['churn_label'].map(mapping)\n",
    "\n",
    "    # Binary Encoding\n",
    "    binary_encoder = BinaryEncoder(cols=['churn_category'])\n",
    "    binary_encoder.fit_transform(df['churn_category'])\n",
    "    churn_cat = binary_encoder.transform(df['churn_category'])\n",
    "    output_df = pd.concat([output_df, churn_cat], axis=1)\n",
    "\n",
    "    # Label Encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    output_df['status'] = label_encoder.fit_transform(df['status'])\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_city_df(df:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the city dataframe as below:\n",
    "    Min-max scales population column\n",
    "    Keep area_id and population only\n",
    "    \"\"\"\n",
    "    # Creating new df\n",
    "    output_df = pd.DataFrame()\n",
    "\n",
    "    # Copying over area_id\n",
    "    output_df['zip_code'] = df['zip_code']\n",
    "\n",
    "    # Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    output_df['population'] = scaler.fit_transform(df[['population']])\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_customer_df(df:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the customer dataframe as below:\n",
    "    Convert following col to 1/0:\n",
    "    - senior_citizen\n",
    "    - married\n",
    "    - gender (1 for male, 0 for female)\n",
    "    \"\"\"\n",
    "    # Create new df\n",
    "    output_df = pd.DataFrame()\n",
    "\n",
    "    # Copying over customer_id, zip_code\n",
    "    output_df['customer_id'] = df['customer_id']\n",
    "    output_df['zip_code'] = df['zip_code']\n",
    "\n",
    "    # Converting columns to 1/0\n",
    "    mapping = {'Yes':1, 'No':0, \"Male\":1, \"Female\":0}\n",
    "    output_df['senior_citizen'] = df['senior_citizen'].map(mapping)\n",
    "    output_df['married'] = df['married'].map(mapping)\n",
    "    output_df['gender'] = df['gender'].map(mapping)\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing all CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dfs from CSV\n",
    "ACCOUNT_DF = pd.read_csv(ACC_PATH)\n",
    "ACCOUNT_USAGE_DF = pd.read_csv(ACC_USAGE_PATH)\n",
    "CHURN_STATUS_DF = pd.read_csv(CHURN_STATUS_PATH)\n",
    "CITY_DF = pd.read_csv(CITY_PATH)\n",
    "CUSTOMER_DF = pd.read_csv(CUSTOMER_PATH)\n",
    "\n",
    "# Preprocessing dfs\n",
    "account_df = preprocess_acc_df(ACCOUNT_DF)\n",
    "account_usage_df = preprocess_acc_usage_df(ACCOUNT_USAGE_DF)\n",
    "churn_status_df = preprocess_churn_df(CHURN_STATUS_DF)\n",
    "city_df = preprocess_city_df(CITY_DF)\n",
    "customer_df = preprocess_customer_df(CUSTOMER_DF)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining customer and account tables on customer_id\n",
    "df = pd.merge(customer_df, account_df, on='customer_id', how='inner')\n",
    "# Joining df with account_usage_df on account_id\n",
    "df = pd.merge(df, account_usage_df, on='account_id', how='inner')\n",
    "# Joining df with churn_status_df on customer_id\n",
    "df = pd.merge(df, churn_status_df, on='customer_id', how='inner')\n",
    "# Joining df with city_df on area_id\n",
    "df = pd.merge(df, city_df, on='zip_code', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai300",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
